tm_map(content_transformer(tolower))
sms_corpus_test <- Corpus(VectorSource(sms_raw_test$text))
sms_corpus_clean_test <- sms_corpus_test %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind='en')) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace) %>%
tm_map(content_transformer(tolower))
## 3.2. 모형설계행렬 ----
### 발생 건수가 5건 이상인것만 추출
# document term matrix
sms_dict <- findFreqTerms(sms_dtm_train, lowfreq=5)
# 학습모델의 입력 데이터 만들기
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
sms_dtm_test <- DocumentTermMatrix(sms_corpus_clean_test)
sms_dtm_train <- DocumentTermMatrix(sms_corpus_clean_train)
## 3.2. 모형설계행렬 ----
### 발생 건수가 5건 이상인것만 추출
# document term matrix
sms_dict <- findFreqTerms(sms_dtm_train, lowfreq=5)
convert_counts <- function(x){
x <- ifelse(x >0, 1, 0)
x <- factor(x, levels = c(0, 1), labels = c("Absent", "Present"))
}
# 행단위로 convert_counts 함수를 적용
sms_train <- sms_train %>% apply(MARGIN=2, FUN=convert_counts)
sms_test <- sms_test %>% apply(MARGIN=2, FUN=convert_counts)
### 참조 사전을 위에서 만든 sms_dict로 사용
sms_train <- DocumentTermMatrix(sms_corpus_clean_train, list(dictionary=sms_dict))
sms_test <- DocumentTermMatrix(sms_corpus_clean_test, list(dictionary=sms_dict))
# 행단위로 convert_counts 함수를 적용
sms_train <- sms_train %>% apply(MARGIN=2, FUN=convert_counts)
sms_test <- sms_test %>% apply(MARGIN=2, FUN=convert_counts)
View(sms_test)
ctrl <- caret::trainControl(method='cv', number=10, repeats=3)
sms_nb_mod <- train(sms_train, sms_raw_train$type, method='nb', trContro=ctrl)
ctrl <- caret::trainControl(method='cv', number=10, repeats=3)
sms_nb_mod <- train(sms_train, sms_raw_train$type, method='nb', trControl=ctrl)
help(train)
# 3.4. 베이즈모형 성능평가 ----
## 새로운 데이터에 대한 spam 여부 결과값
sms_nb_pred <- predict(sms_nb_mod, sms_test)
## 새로운 데이터에 대한 spam 여부 결과값
cm_nb <- confusionMatrix(sms_nb_pred, sms_raw_test$type, positive='spam')
cm_nb
inSpam <- 4/80 * 10/80 * 1/80 * 12/80
inHam <- 1/320 * 14/320 * 9/320 * 23/320
inSPam
inSpam
inHam
inSpam *= (80/400)
inSpam = inSpam * (80/400)
inHam = inHam * (320/400)
inSpam
inHam
inSpam <- a / (a + b)
inHam <- b / (a + b)
a <- inSpam; b <- inHam;
inSpam <- a / (a + b)
inHam <- b / (a + b)
inSpam
inHam
library(tidyverse)
library(caret)
library(tm) # library for text mining
library(extrafont)
setwd("./naive_bayse")
sms_raw <- read.csv("sms_spam.csv", encoding="UTF-8")
sms_raw$type <- factor(sms_raw$type)
train_index <- caret::createDataPartition(sms_raw$type, p=0.75, list=FALSE)
# sms_raw$type이 균일하게 분포하도록 row number를 뽑아 준다.
# p는 어느 정도의 비율을 추출할지를 나타낸다.
# train data
sms_raw_train <- sms_raw[train_index,]
# test data
sms_raw_test <- sms_raw[-train_index,]
# create train corpus and refine, and make Document Term Matrix
sms_corpus_clean_train <- Corpus(VectorSource(sms_raw_train$text)) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind='en')) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
sms_dtm_train <- DocumentTermMatrix(sms_corpus_clean_train)
sms_dict <- findFreqTerms(sms_dtm_train, lowfreq=5)
sms_dtm_train <- DocumentTermMatrix(sms_corpus_clean_train, list(dictionary=sms_dict))
# create train corpus and refine, and make Document Term Matrix
sms_corpus_clean_test <- Corpus(VectorSource(sms_raw_test$text)) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind='en')) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
sms_dtm_test <- DocumentTermMatrix(sms_corpus_clean_test, list(dictionary=sms_dict))
# labeling function
convert_corpus <- function(x){
x <- ifelse(x > 0 , 1, 0)
x <- factor(x, levels=c(0, 1), labels=c("Absent", "Present"))
}
# label
sms_train <- sms_dtm_train %>% apply(MARGIN=2,  FUN=convert_corpus)
sms_test <- sms_dtm_test %>% apply(MARGIN=2, FUN=convert_corpus)
# Modeling
# trainControl : Control the computational nuances of the train function
ctrl <- trainControl(method='cv', number=10) # cross validation 10 times. repeat = 3 이 안되네..
sms_nb_mod <- train(sms_train, sms_raw_train$type, method='nb', trControl=ctrl)
# Evaluation
(sms_nb_mod)
sms_nb_pred <- predict(sms_nb_mod, sms_test)
(cm_nb <- confusionMatrix(sms_nb_pred, sms_raw_test$type, positive='spam'))
as.character(sms_corpus[[1]])
음파파 음파파
음파파 음파파
음파파 음파ㅏ
음파음파``
as.character(sms_corpus_clean_train[[1]])
install.packages("snowballC")
install.packages("SnowballC")
ry(sS)
library(SnowballC)
# create train corpus and refine, and make Document Term Matrix
sms_corpus_clean_train <- Corpus(VectorSource(sms_raw_train$text)) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind='en')) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace) %>%
tm_map(stemDocument)
sms_dtm_train <- DocumentTermMatrix(sms_corpus_clean_train)
as.character(sms_corpus_clean_train[[1]])
library(worldcloud)
library(wordcloud)
wordcloud(sms_corpus_clean_train, min.freq=50, random.order=FALSE)
sms_dtm_train <- DocumentTermMatrix(sms_corpus_clean_train)
sms_dict <- findFreqTerms(sms_dtm_train, lowfreq=5)
sms_dtm_train <- DocumentTermMatrix(sms_corpus_clean_train, list(dictionary=sms_dict))
# create train corpus and refine, and make Document Term Matrix
sms_corpus_clean_test <- Corpus(VectorSource(sms_raw_test$text)) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind='en')) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace) %>%
tm_map(stemDocument)
wordcloud(sms_corpus_clean_test, min.freq=50, random.order=FALSE)
sms_dtm_test <- DocumentTermMatrix(sms_corpus_clean_test, list(dictionary=sms_dict))
# labeling function
convert_corpus <- function(x){
x <- ifelse(x > 0 , 1, 0)
x <- factor(x, levels=c(0, 1), labels=c("Absent", "Present"))
}
# label
sms_train <- sms_dtm_train %>% apply(MARGIN=2,  FUN=convert_corpus)
sms_test <- sms_dtm_test %>% apply(MARGIN=2, FUN=convert_corpus)
sms_raw
colnames(sms)
colnames(sms_raw)
spam <- sms_raw %>% filter(type=='spam')
ham <- sms_raw %>% filter(type=='ham')
wordcloud(spam$text, min.freq=10, random.order=FALSE)
wordcloud(ham$text, max.words=40, random.order=FALSE)
wordcloud(spam$text, min.freq=10, random.order=FALSE)
# 빈발단어만 추리기
sms_freq_word <- findFreqTemrs(sms_dtm_train)
# 빈발단어만 추리기
sms_freq_word <- findFreqTerms(sms_dtm_train, 5)
sms_freq_word
str(sms_freq_word)
sms_dtm_train <- sms_dtm_train[,sms_freq_words]
# 빈발단어만 추리기
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
sms_dtm_train <- sms_dtm_train[,sms_freq_words]
sms_dtm_train
dim(sms_dtm_train)
library(tidyverse)
library(caret)
library(tm) # library for text mining
library(extrafont)
library(SnowballC) # stemming(learning, learned => learn)
library(wordcloud)
rm(list=ls(all.names=T))
library(tidyverse)
library(caret)
library(tm) # library for text mining
library(extrafont)
library(SnowballC) # stemming(learning, learned => learn)
library(wordcloud)
sms_raw <- read.csv("sms_spam.csv", encoding="UTF-8")
train_index <- caret::createDataPartition(sms_raw$type, p=0.75, list=FALSE)
sapply(sms_raw, class)
table(sms_raw$type)
sms_raw[type!='ham' && type!='spam']
sms_raw[type!='ham' && type!='spam',]
sms_raw[sm$type!='ham' && sms$type!='spam',]
sms_raw[sms$type!='ham' && sms$type!='spam',]
sms_raw[sms_raw$type!='ham' && sms_raw$type!='spam',]
# create train corpus and refine, and make Document Term Matrix
sms_corpus <- Corpus(VectorSource(sms_raw$text)) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind='en')) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace) %>%
tm_map(stemDocument)
as.character(sms_corpus_clean[[1]])
# create train corpus and refine, and make Document Term Matrix
sms_corpus_clean <- Corpus(VectorSource(sms_raw$text)) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind='en')) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace) %>%
tm_map(stemDocument)
as.character(sms_corpus_clean[[1]])
# 빈발단어만 추리기
wordcloud(sms_corpus_clean, min.freq=50, random.order=FALSE)
# create document term matrix
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
inspect(sms_dtm[1:10, 1:30])
sms_freq_words <- findFreqTerms(sms_dtm, 5)
str(sms_freq_words)
sms_dtm_freq <- sms_dtm[,sms_freq_words]
dim(sms_dtm_freq)
dim(sms_dtm)
# labeling function
convert_corpus <- function(x){
x <- ifelse(x > 0 , 1, 0)
x <- factor(x, levels=c(0, 1), labels=c("Absent", "Present"))
}
library(e1072)
library(e1072)
library(e1071)
rm(list=ls(all.names=# Fri Nov 15 14:14:58 2019 ------------------------------
))
;
library(tidyverse)
library(caret)
library(tm) # library for text mining
library(extrafont)
library(SnowballC) # stemming(learning, learned => learn)
library(wordcloud)
library(e1071)
setwd("./naive_bayse")
sms_raw <- read.csv("sms_spam.csv", encoding="UTF-8")
table(sms_raw$type)
# create train corpus and refine
sms_corpus_clean <- Corpus(VectorSource(sms_raw$text)) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind='en')) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace) %>%
tm_map(stemDocument)
as.character(sms_corpus_clean[[1]])
# wordcloud for ham, spam
wordcloud(sms_corpus_clean, min.freq=50, random.order=FALSE)
ham <- subset(sms_raw, sms_raw$type=='ham')
spam <- subset(sms_raw, sms_raw$type=='spam')
wordcloud(ham$text, min.freq=50, random.order=FALSE)
wordcloud(spam$text, min.freq=10, random.order=FALSE)
wordcloud(spam$text, max.words=40, random.order=FALSE)
# create document term matrix
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
inspect(sms_dtm[1:10, 1:30])
sms_freq_words <- findFreqTerms(sms_dtm, 5)
str(sms_freq_words)
sms_dtm_freq <- sms_dtm[,sms_freq_words]
dim(sms_dtm_freq)
dim(sms_dtm)
# labeling function
convert_counts <- function(x){
x <- ifelse(x > 0 , 1, 0)
x <- factor(x, levels=c(0, 1), labels=c("Absent", "Present"))
}
# label
sms_dtm_convert <- apply(sms_dtm_freq, 2, convert_counts)
# data partitioning
train_index <- createDataPartition(sms_raw$type, p=0.75, list=FALSE)
sms_dtm_train <- sms_dtm_convert[train_index,]
sms_raw_train <- sms_raw[train_index.]
sms_dtm_test <- sms_dtm_convert[-train_index,]
sms_raw_test <- sms_raw[-train_index,]
# Modeling
## Naive Bayes at e1071
m <- naiveBayes(sms_dtm_train, sms_raw_train$type)
# Evaluation
p <- predict(m, sms_dtm_test, type='class')
table(sms_raw_test$type, p)
sms_raw_train <- sms_raw[train_index,]
# Modeling
## Naive Bayes at e1071
m <- naiveBayes(sms_dtm_train, sms_raw_train$type)
# Evaluation
p <- predict(m, sms_dtm_test, type='class')
table(sms_raw_test$type, p)
library(gmodels)
CrossTable(p, sms_raw_test$type,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
table(sms_raw_test$type, p)
CrossTable(p, sms_raw_test$type,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
?naiveBayes
library(tidyverse)
library(caret)
library(tm) # library for text mining
library(extrafont)
library(SnowballC) # stemming(learning, learned => learn)
library(wordcloud)
library(e1071)
library(gmodels)
setwd("./naive_bayse")
sms_raw <- read.csv("sms_spam.csv", encoding="UTF-8")
table(sms_raw$type)
# create train corpus and refine
sms_corpus_clean <- Corpus(VectorSource(sms_raw$text)) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind='en')) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace) %>%
tm_map(stemDocument)
as.character(sms_corpus_clean[[1]])
# wordcloud for ham, spam
wordcloud(sms_corpus_clean, min.freq=50, random.order=FALSE)
ham <- subset(sms_raw, sms_raw$type=='ham')
spam <- subset(sms_raw, sms_raw$type=='spam')
wordcloud(ham$text, min.freq=50, random.order=FALSE)
wordcloud(spam$text, min.freq=10, random.order=FALSE)
wordcloud(spam$text, max.words=40, random.order=FALSE)
# create document term matrix
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
inspect(sms_dtm[1:10, 1:30])
sms_freq_words <- findFreqTerms(sms_dtm, 5)
str(sms_freq_words)
sms_dtm_freq <- sms_dtm[,sms_freq_words]
dim(sms_dtm_freq)
dim(sms_dtm)
# labeling function
convert_counts <- function(x){
x <- ifelse(x > 0 , 1, 0)
x <- factor(x, levels=c(0, 1), labels=c("Absent", "Present"))
}
# label
sms_dtm_convert <- apply(sms_dtm_freq, 2, convert_counts)
# data partitioning
train_index <- createDataPartition(sms_raw$type, p=0.75, list=FALSE)
sms_dtm_train <- sms_dtm_convert[train_index,]
sms_raw_train <- sms_raw[train_index,]
sms_dtm_test <- sms_dtm_convert[-train_index,]
sms_raw_test <- sms_raw[-train_index,]
# Modeling
## Naive Bayes at e1071
m <- naiveBayes(sms_dtm_train, sms_raw_train$type)
m <- naiveBayes(sms_dtm_train, sms_raw_train$type, laplace = 2)
# Evaluation
p <- predict(m, sms_dtm_test, type='class')
table(sms_raw_test$type, p)
CrossTable(p, sms_raw_test$type,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
## Naive Bayes at caret
### trainControl : Control the computational nuances of the train function
ctrl <- trainControl(method='cv', number=10) # cross validation 10 times. repeat = 3 이 안되네..
sms_nb_mod <- train(sms_train, sms_raw_train$type, method='nb', trControl=ctrl)
# Evaluation
(sms_nb_mod)
sms_nb_pred <- predict(sms_nb_mod, sms_test)
(cm_nb <- confusionMatrix(sms_nb_pred, sms_raw_test$type, positive='spam'))
# Kappa Statistic
# 우연히 맞을 확률을 제외하여 평가
# K = (Pr(a) - Pr(e)) / (1 - Pr(e))
# Pr(a) = 분류기가 실제 일치하게 맞추는 비율
# Pr(e) = 분류기 값과 실제값이 무작위 선택되었다고 가정했을때 두 값 사이의 예상 일치
# Pr(e) = (실제 양성의 비율) * (전체 데이터 비 분류기가 양성으로 분류한 비율) + (실제 음성의 비율) * (전체 데이터 비 분류기가 음성으로 분류한 비율)
(cm_nb <- ConfusionMatrix(sms_nb_pred, sms_raw_test$type, positive='spam'))
(cm_nb <- confusionMatrix(sms_nb_pred, sms_raw_test$type, positive='spam'))
sms_nb_pred <- predict(sms_nb_mod, sms_test)
sms_nb_mod <- train(sms_train, sms_raw_train$type, method='nb', trControl=ctrl)
CrossTable(p, sms_raw_test$type,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
sms_nb_mod <- train(sms_dtm_train, sms_raw_train$type, method='nb', trControl=ctrl)
(sms_nb_mod)
sms_nb_pred <- predict(sms_nb_mod, sms_test)
(cm_nb <- confusionMatrix(sms_nb_pred, sms_raw_test$type, positive='spam'))
# Kappa Statistic
# 우연히 맞을 확률을 제외하여 평가
# K = (Pr(a) - Pr(e)) / (1 - Pr(e))
# Pr(a) = 분류기가 실제 일치하게 맞추는 비율
# Pr(e) = 분류기 값과 실제값이 무작위 선택되었다고 가정했을때 두 값 사이의 예상 일치
# Pr(e) = (실제 양성의 비율) * (전체 데이터 비 분류기가 양성으로 분류한 비율) + (실제 음성의 비율) * (전체 데이터 비 분류기가 음성으로 분류한 비율)
# F - 척도
# 모델의 서능을 정밀도와 재현율을 하나의 값으로 평가하고자 할때 사용
# 정밀도와 재현율을 조화평균내서 사용함
# F - 척도 = (2 * 정밀도 * 재현율) / (재현율 + 정밀도)
# 두 메트릭을 하나의 메트릭으로 만들어 사용
# Evaluation
(sms_nb_mod)
(cm_nb <- confusionMatrix(sms_nb_pred, sms_raw_test$type, positive='spam'))
(cm_nb <- confusionMatrix(sms_nb_pred, sms_raw_test$type, positive='spam'))
sms_nb_pred <- predict(sms_nb_mod, sms_test)
(cm_nb <- confusionMatrix(sms_nb_pred, sms_raw_test$type, positive='spam'))
sms_nb_pred <- predict(sms_nb_mod, sms_test)
sms_nb_pred <- predict(sms_nb_mod, sms_dtm_test)
(cm_nb <- confusionMatrix(sms_nb_pred, sms_raw_test$type, positive='spam'))
(cm_nb <- confusionMatrix(sms_nb_pred, sms_raw_test$type, positive='spam'))
# 민감도 (sensitivity, true positive rate)
# 관심있는 클래스를 얼마나 잘 분류해 내는지
# sensitivity = (TP) / (TP + FN)
caret::sensitivity(sms_results$predict_type, sms_result$actual_type, positive="spam")
library(ROCR)
sms_test_prob <- predict(m, sms_test, type='raw')
sms_test_prob <- predict(m, sms_dtm_test, type='raw')
pred <- prediction(predictions = sms_test_prob[,2],
labels = sms_raw_test$type)
sms_test_prob
sms_test_prob <- predict(m, sms_dtm_test)
pred <- prediction(predictions = sms_test_prob[,2],
labels = sms_raw_test$type)
length(sms_raw_test$type)
length(sms_dtm_test)
length(sms_test_prob)
sms_test_prob[,2]
sms_test_prob[22]
sms_test_prob[2]
sms_test_prob
sms_test_prob <- predict(m, sms_dtm_test, type='raw')
sms_test_prob
library(tidyverse)
library(tm)
library(SnowballC)
sms_raw <- read.csv("sms_spam.csv", stringsAsFactors=FALSE, encoding='UTF-8')
table(sms_raw$type) # ham 4812, spam 747
sms_raw$type <- factor(sms_raw$type)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
sms_corpus_clean <- sms_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind='en')) %>% # 의미 분석에 큰 의미가 없는 불용어(I, my, me, over)제거
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
# check
as.character(sms_corpus_clean[[1]])
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
as.character(sms_corpus_clean[[1]])
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq=50, random.order=FALSE)
ham <- subset(sms_raw, sms_raw$type=='ham')
spam <- subset(sms_raw, sms_raw$type=='spam')
wordcloud(ham$text, min.freq=50, random.order=FALSE)
wordcloud(spam$text, min.freq=10, random.order=FALSE)
wordcloud(spam$text, max.words=10, random.order=FALSE)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
inspect(sms_dtm[1:10, 1:30])
inspect(sms_dtm[1:11, 1:30])
inspect(sms_dtm[1:12, 1:30])
sms_freq_words <- findFreqTerms(sms_dtm, 5)
sms_freq_words
?findFreqTerms
str(sms_freq_words)
sms_dtm_freq <- sms_dtm[,sms_freq_words]
dim(sms_dtm_freq)
dim(sms_dtm)
# labeling function
convert_counts <- function(x){
x <- ifelse(x > 0, 1, 0)
x <- factor(x, levels=c(0, 1), labels=c("Absent", "Present"))
}
# label
sms_dtm_convert <- apply(sms_dtm_freq, 2, convert_counts)
# data partitioning
train_index <- createDataPartition(sms_raw$type, p=0.75, list=FALSE)
sms_dtm_train <- sms_dtm_convert[train_index,]
sms_dtm_test <- sms_dtm_convert[-train_index,]
sms_raw_train <- sms_raw[train_index,]
sms_raw_test <- sms_raw[-train_index,]
inspect(sms_dtm_train)
inspect(sms_dtm_train[1:10, 1:30])
View(sms_dtm_test)
library(e1071)
m <- naiveBayes(sms_dtm_train, sms_raw_train$type)
p <- predict(m, sms_raw_test, type='class')
table(sms_raw_test$type, p)
table(p)
length(sms_raw_test)
p <- predict(m, sms_raw_test$text, type='class')
table(p)
sms_raw_test$text
p <- predict(m, sms_dtm_test, type='class')
table(p)
library(gmodels)
CrossTable(sms_test_pred, sms_test_labels,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
CrossTable(p, sms_raw_test$type,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
sms_nb_mod <- train(sms_dtm_train, sms_raw_train$type, method='nb', trControl = ctrl)
(sms_nb_mod)
sms_nb_pred <- predict(sms_nb_mod, sms_dtm_test)
(cm_nb <- confusionMatrix(sms_nb_pred, sms_raw_test$type, positive='spam'))
library(ROCR)
sms_test_prob <- predict(m, sms_dtm_test, type='raw')
pred <- prediction(predictions = sms_test_prob[,2],
labels = sms_raw_test$type)
sms_test_prob[,2]
perf <- performance(pred, measure = 'tpr', x.measure = 'fpr')
sms_test_prob <- predict(m, sms_dtm_test, type='class')
pred <- prediction(predictions = sms_test_prob,
labels = sms_raw_test$type)
