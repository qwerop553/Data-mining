col=1
row=row+1
}
#상관계수 값이 0으로 검정한 값들만 가져오기
zero.mat<-zero.mat>0.05
#데이터프레임으로 변환
cor.df<-data.frame(zero.mat)
#변수 추출
feature <- c("all_time","max","min","std","calorie","stop_time")
#feature <- c("all_time", "max", "mean", "median", "min",
#             "std", "calorie", "work", "distance", "floor",
#             "stop_time", "sat", "pat", "vat", "active_calorie")
data <- data[c(feature,"sleep_score")]
#train, test을 나누는 함수
split <- function(data,k){
set.seed(k*1000)
indexTrain <- sample(1:nrow(data), round(nrow(data) * .7))
return(indexTrain)
}
#단층 퍼셉트론 모델
model_nnet <- function(train){
set.seed(20171493)
model <- nnet(sleep_score ~ ., data=train, size=90, linout=T, maxit=5000, decay=0.05, rang=0.7)
return(model)
}
#RMSE 값을 넣을 빈 벡터 생성
nnet_rmse_value <- c()
#중복을 허용한 교차검증 (100번 수행)
for (i in 1:5){
#데이터 파티셔닝 (시드에 난수를 발생시켜 파티셔닝이 일정하지 않도록 함)
indexTrain <- split(data,i)
train <- data[indexTrain,]
test <- data[-indexTrain,]
#모델
nnet_model <- model_nnet(train)
#인공신경망 결과
#summary(nnet_model)
#인공신경망 plot
#plotnet(nnet_model)
#각 변수의 중요도
#garson(nnet_model)
#fitted, train 수면 점수
#nnet_ft_score <- data.frame(fitted=nnet_model$fitted.values,
#                            train=train$sleep_score)
#predict, test 수면 점수
nnet_pt_score <- data.frame(predict=predict(nnet_model, newdata=test[feature], type="raw"),
test=test$sleep_score)
#RMSE
nnet_rmse_value <- c(nnet_rmse_value, RMSE(nnet_pt_score$test, nnet_pt_score$predict))
}
mean(nnet_rmse_value)
sd(nnet_rmse_value)
#파일 경로 설정 (성원)
setwd("C:/Users/user/Desktop/대회/data/csw")
#파일 불러오기
active_hyc <- readRDS("active_hyc")
fact_csw <- readRDS("fact_csw")
fake_csw <- readRDS("fake_csw")
#fitbit 기기의 정확도
round(mean(100-fake_csw$sleep_time/fake_csw$all_time),2)
ifelse("a", cat("he"), cat("de"))
isTRUE("A")
isTRUE("Adf")
library(stringr)
str_extract("abcdefg", pattern="abc(de)f")
a <- str_extract("abcdefg", pattern="abc(de)f")
a
stri_match_all_regex(c("breakfast=eggs;lunch=pizza",
"breakfast=bacon;lunch=spaghetti", "no food here"),
"(\\w+)=(\\w+)")
library(stringi)
stri_match_all_regex(c("breakfast=eggs;lunch=pizza",
"breakfast=bacon;lunch=spaghetti", "no food here"),
"(\\w+)=(\\w+)")
str.split("경상남도 거제시 동푸면")
str_split("경상남도 거제시 동푸면 똥맨디")
str_split("경상남도 거제시 동푸면 똥맨디", pattern=' ')
str_split("경상남도 거제시 동푸면 똥맨디", pattern=' ')[1]
str_split("경상남도 거제시 동푸면 똥맨디", pattern=' ')[1][[1]]
str_split("경상남도 거제시 동푸면 똥맨디", pattern=' ')[1][1]
str_split("경상남도 거제시 동푸면 똥맨디", pattern=' ')[2]
str_split("경상남도 거제시 동푸면 똥맨디", pattern=' ')[[1]][2]
#라이브러리 호출
library(nnet)
a <- c(1, 2, 3, 4)
a <- c(1, 0, 0, 1)
b <- c('a', 'b', 'a', 'b')
table(a, b)
m <- table(a, b)
m[2,2]
m[4]
m[2, 2] / sum(m[,2])
m[2, 2] / sum(m[4]
m[2, 2] / sum(m[4])
m[2, 2] / sum(m[4])
m[2, 2] /(m[3] + m[4])
A <- c(1, 0, 0, 1, 0)
B <- C(0, 0, 1, 1, 0)
(m <- table(A, B))
A <- c(0, 1, 1, 0, 0, 0, 1, 1, 1, 1)
B <- C(0, 0, 0, 1, 1, 1, 1, 1, 1, 1)
(m <- table(A, B))
A <- c(0, 1, 1, 0, 0, 0, 1, 1, 1, 1)
A <- c(0, 1, 1, 0, 0, 0, 1, 1, 1, 1)
B <- c(0, 0, 0, 1, 1, 1, 1, 1, 1, 1)
(m <- table(A, B))
m[0]
m[1]
m[2]
m[3]
m[4]
inSpam <- (4/20) * (10/20) * (0/20) * (12/20)
inHam <- (1/80) * (14/80) * (8/79) * (23/80)
inSpam
inHam
pw <- (5/100) * (24/100) * (8/100) * (35/100)
inSpam <- ((4/20) * (10/20) * (0/20) * (12/20)) / pw
inHam <- ((1/80) * (14/80) * (8/79) * (23/80)) / pw
inSpam
inHam
A <- c(0, 1, 1, 0, 0, 0, 1, 1, 1, 1)
B <- c(0, 0, 0, 1, 1, 1, 1, 1, 1, 1)
(m <- table(A, B))
m[1]
m[2]
m[3]
m[4]
pw <- (5/100) * (24/100) * (8/100) * (35/100)
inSpam <- ((4/20) * (10/20) * (0/20) * (12/20) * (20/100)) / pw
inHam <- ((1/80) * (14/80) * (8/79) * (23/80) * (80/100)) / pw
inSpam
inHam
# 우도 - 스팸일 때
(a1 <- (4/20))  # 당일
(a2 <- (10/20)) # 최고
(a3 <- (0/20))  # 최저
(a4 <- (12/20)  # 팀장
# 우도 - 햄일 때
(b1 <- (1/80))  # 당일
(b2 <- (14/80)  # 최고
(b3 <- (8/79))  # 최저
(b4 <- (23/80)) # 팀장
# 우도 - 스팸일 때
(a1 <- (4/20))  # 당일
(a2 <- (10/20)) # 최고
(a3 <- (0/20))  # 최저
(a4 <- (12/20)) # 팀장
# 우도 - 햄일 때
(b1 <- (1/80))  # 당일
(b2 <- (14/80)  # 최고
(b2 <- (14/80)) # 최고
(b3 <- (8/79))  # 최저
(b4 <- (23/80)) # 팀장
# 판단
# 스팸일까
inSpam <- a1 * a2 * a3 * a4 * (20/100)
inHam <- b1 * b2 * b3 * b4 * (80/100)
inSpam < inHam
if (inSpam < inHam) cat("Ham")
else cat("Spam")
if (inSpam < inHam){ cat("Ham")}
else{ cat("Spam")}
if (inSpam < inHam){ cat("Ham")}else{ cat("Spam")}
if (inSpam > inHam){ cat("Ham")}else{ cat("Spam")}
pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species")
pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",
pch=21, bg = c("red", "green3", "blue")[unclass(iris$Species)])
unclass(iris$Species)
class(iris$Species)
unclass(iris$Species)
unclass(iris$Species)[1]
c("red", "green3", "blue")[unclass(iris$Species)]
height <- c(182, 160, 165, 170, 163, 160, 181, 166, 159, 145, 175)
hist(height)
boxplot(height)
hist(height, breaks=3)
hist(height, breaks=4)
hist(height, breaks=5)
hist(height, breaks=6)
hist(height, breaks=7)
hist(height, breaks=8)
hist(height, breaks=9)
hist(height, breaks=10)
hist(height, breaks=4)
hist(height, breaks=5)
hist(height, breaks=3)
hist(height, breaks=6)
hist(height, breaks=5)
hist(height, breaks=3)
hist(height, probability = T)
hist(height, probability = T, main="학생성적", ylim=c(0, 0.04))
boxplot(height)
library(reshape2)
reshape2::cast
reshape::cas
reshape::cast
install.packages("reshape")
reshape::cast
data(airquality)
head(airquality)
head(airquality, 3)
T <- melt(airquality, id=c("month", "day"))
T <- melt(airquality, id=c("Month", "Day"))
T
cast(T, day~month~variable)
library(reshape)
cast(T, day~month~variable)
cast(T, Day~Month~variable)
b <- acast(T, month~variable, mean)
b <- acast(T, Month~variable, mean)
b
T <- melt(airquality, id=c("Month", "Day"), na.rm=T)
T <- melt(airquality, id=c("Month", "Day"), na.rm=TRUE)
T
cast(T, Day~Month~variable)
b <- acast(T, Month~variable, mean)
b
d <- cast(T, month~variable, mean, margins = c("grand_row", "grand_col"))
d <- cast(T, Month~variable, mean, margins = c("grand_row", "grand_col"))
d
e <- cast(T, Day~Month, mean, subset=variable=='ozone')
e <- cast(T, Day~Month, mean, subset=variable=='ozone')
T
e <- cast(T, Day~Month, mean, subset=variable=='Ozone')
e
f <- cast(T, Month~variable, range)
f
e <- cast(T, Day~Month, mean, subset=variable=='Ozone')
e
?cast
install.packages("sqldf")
library(sqldf)
data(iris)
sqldf("select * from iris")
sqldf("select * from iris where Species=setosa")
sqldf("select * from iris where Species='setosa'")
sqldf("select count(*) from iris where species like 'se%'")
set.seed(1)
d <- data.frame(year=rep(2012:2014, each=6), count=runif(9, 0, 20))
d
d <- data.frame(year=rep(2012:2014, each=6), count=as.integer(runif(9, 0, 20)))
d
library(plyr)
data.frame(cv.count=cv)
d
ddply(d, "year", function(x){
mean.count=mean(x$count)
sd.count=sd(x$count)
cv = sd.count/mean.count
data.frame(cv.count=cv)
})
ddply(d, .(year), summarise, mean.count=mean(count))
.(year)
ddply(d, .(year), transform, total.count=sum(count))
ddply(d, .(year), transform, total.count=sum(count))
library(data.table)
DT <- data.table(x=c("b", "b", "b", "a", "a"), v=rnorm(5))
DT
str(cars)
CARS <- as.data.table(cars)
head(CARS)
tables()
DT
CARS
sapply(CARS, class)
setkey(DT, x)
tables()
DT['b']
CARS['speed']
CARS
setkey(CARS, speed)
CARS['19']
CARS[19]
tables()
y <- c(1, 2, 3, NA)
is.na(y)
data(iris)
iris[iris$Petal.Width==0.2, "Petal.Width"] <- NA
is.na(iris$Petal.Width)
x <- c(1, 2, NA, 3)
mean(x)
mean(x, na.rm=T)
mean(x, na.rm=TRUE)
rm(T)
data("french_fries")
?french_fries
french_fries[!complete.cases(french_fries),]
library(Amelia)
data(freetrade)
data(Freetrade)
data(FreeTrade)
?freetrade
??freetrade
?amelia
?amelia
install.packages("Amelia")
library(Amelia)
data(freetrade)
head(freetrade)
a.out <- amelia(freetrade, m=5, ts='year', cs='country')
?amelia
hist(a.out$imputations[[3]]$tariff, col='grey', border='white')
missmap(a.out)
freetrade$tariff <- a.out$imputations[[5]]$tariff
missmapp(freetrade)
mismapp(freetrade)
missmap(freetrade)
install.packages("outliers")
library(outlier)
set.seed(1234)
y=rnorm(100)
outlier(1y)
outlier(y)
library(outlier)
library(outliers)
outlier(y)
outlier(y, opposite = TRUE)
y
dim(y) <- c(20, 5)
y
outlier(y)
outlier(y, opposite=TRUE)
boxplot(y)
x <- "SEOUL"
mode(x)
?mode
a1 <- c(2, 7, 4)
a2 <- 1:4
a1 - a2
?tree::tree
install.packages("tree")
?tree:tree
?tree::tree
??tree::tree
??tree
?tree
??tree
setwd("c:/data_mining/knn")
dir()
wbcd <- read.csv("wisc_bc_data.csv",
stringsAsFactors = FALSE)
str(wbcd)
wbcd <- wbcd[-1]
table(wbcd$diagnosis)
wbcd$diagnosis <- factor(wbcd$diagnosis, levels=c("B", "M"),
c("Benign", "Maligant"))
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
return ((x - min(X)) / (max(x) - min(x)))
normalize <- function(x){
return ((x - min(x)) / (max(x) - min(x)))
}
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
# normalize the wbcd data
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
wbcd[2]
wbcd[1]
wbcd[31]
wbcd[32]
summary(wbcd_n$area_mean)
index <- sample(1:nrow(wbcd_n), nrow(wbcd_n) * 0.7)
wbcd_train <- wbcd_n[index,]
wbcd_test <- wbcd_n[-index,]
wbcd_train_labels <- wbcd[index, 1]
wbcd_test_labels <- wbcd[-index, 1]
library(class)
wbcd_test_pred <- knn(train = wbcd_train,
test = wbcd_test,
cl = wbcd_train_labels,
k = 21)
wbcd_test_pred
nrow(test)
nrow(wbcd_test)
library(gmodels)
CrossTable(x = wbcd_test_labels,
y = wbcd_test_pred,
prop.chisq = FALSE)
president <- read.csv("US Presidential Data.csv")
president$Win.Loss
president$Win.Loss <- as.factor(president$Win.Loss)
table(president$Win.Loss)
president$Win.Loss = ifelse(president$Win.Loss==1, "win", "loss")
president$Win.Loss
library(caret)
index = createDataPartition(president$Win.Loss, p = 0.7, list = F)
trian = president[index,]
train = president[index,]
train = president[index,]
validation = president[-index,]
set.seed(123)
x = trainControl(method = 'repeatedcv',
number = 10,
repeats = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary)
model <- trian(Win.Loss~., data=train, method='knn',
preProcess=c("center", "scale"),
trControl = x, metric="ROC", tuneLength = 10)
model <- train(Win.Loss~., data=train, method='knn',
preProcess=c("center", "scale"),
trControl = x, metric="ROC", tuneLength = 10)
model
plot(model)
model <- train(Win.Loss~., data=train, method='knn',
preProcess=c("center", "scale"),
trControl = x, metric="ROC", tuneLength = 10)
model
plot(model)
valid.pred <- predict(model, validation, type='prob')
valid.pred
apply(valid.pred, 1, which.max)
model <- train(Win.Loss~., data=train, method='rpart',
preProcess=c("center", "scale"),
trControl = x, metric="ROC", tuneLength = 10)
model
library(tidyverse)
library(caret)
library(tm)
sms_raw <- read.csv("sms_spam.csv")
dir()
cd ..
setwd("..")
dir()
setwd("./naive_bayse")
dir()
sms_raw <- read.csv("sms_spam.csv")
sms_raw$type <- factor(sms_raw$type)
table(sms_raw$type)
which(sms_raw$type == 'All done, all handed in. Don't know if mega shop in asda counts as celebration but thats what i'm doing!')
which(sms_raw$type == "All done, all handed in. Don't know if mega shop in asda counts as celebration but thats what i'm doing!")
sms_raw <- sms_raw[-1072,]
talbe(sms_raw$type)
table(sms_raw$type)
sms_raw$type <- factor(sms_raw$type)
table(sms_raw$type)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
sms_corpus_clean <- sms_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind='en')) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
sms_raw <- read.csv("sms_spam.csv", encoding='utf8')
sms_raw$type <- factor(sms_raw$type)
table(sms_raw$type)
which(sms_raw$type == "All done, all handed in. Don't know if mega shop in asda counts as celebration but thats what i'm doing!")
sms_raw <- sms_raw[-1072,]
table(sms_raw$type)
sms_raw$type <- factor(sms_raw$type)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
sms_corpus_clean <- sms_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind='en')) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
sms_raw <- read.csv("sms_spam.csv", encoding='UTF-8')
sms_raw$type <- factor(sms_raw$type)
table(sms_raw$type)
which(sms_raw$type == "All done, all handed in. Don't know if mega shop in asda counts as celebration but thats what i'm doing!")
sms_raw <- sms_raw[-1072,]
table(sms_raw$type)
sms_raw$type <- factor(sms_raw$type)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
sms_corpus_clean <- sms_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind='en')) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
train_index <- createDataPartition(sms_raw$type, p=0.75, list=FALSE)
sms_raw_train <- sms_raw[train_index,]
sms_raw_test <- sms_raw[-train_index,]
sms_dict <- findFreqTerms(sms_dtm_train, lowfreq=5)
sms_corpus[1]
sms_corpus[[1]]
as.character(sms_corpus[[1]])
library(SnowballC)
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
spam <- subset(sms_raw, sms_raw$type=='spam')
wordcloud(ham$text, min.freq=50, random.order = FALSE)
ham <- subset(sms_raw, sms_raw$type=='ham')
spam <- subset(sms_raw, sms_raw$type=='spam')
wordcloud(ham$text, min.freq=50, random.order = FALSE)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
inspect(sms_dtm[1:10, 1:30])
sms_freq_words <- findFreqTerms(sms_dtm, 5)
sms_freq_words
sms_dtm_freq <- sms_dtm[,sms_freq,words]
sms_dtm_freq <- sms_dtm[,sms_freq_words]
dim(sms_dtm_freq)
dim(sms_dtm)
convert_counts <- function(x){
x <- ifelse(x > 0, 1, 0)
x <- factor(x, levels=c(0, 1), labels = c("Absent", "Present"))
}
sms_dtm_convert <- apply(sms_dtm_freq, convert_counts)
sms_dtm_convert <- apply(sms_dtm_freq, 1, convert_counts)
sms_dtm_convert
inspect(sms_dtm_convert[1:50,])
inspect(sms_dtm_convert[1:10,])
train_index <- createDataPartition(sms_raw$type, p=0.75, list=FALSE)
sms_dtm_train <- sms_dtm_convert[train_index,]
sms_dtm_test <- sms_dtm_convert[-train_index,]
train_index <- createDataPartition(sms_raw$type, p=0.75, list=FALSE)
sms_dtm_train <- sms_dtm_convert[train_index,]
sms_dtm_test <- sms_dtm_convert[-train_index,]
