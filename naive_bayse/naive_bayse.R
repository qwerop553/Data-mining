# 나이브 베이즈 모델의 유례
#
# 18세기 수학자 토마스 베이즈의 연구소에서 유래
# 베이지안(Baysian) 기법 군을 형성
# - 훈련 데이터의 특징 값이 제공하는 증거를 기반으로 결과가 관측될 확률을 계산
# - 테스트 데이터에 적용될 때 결과가 관측될 확률을 이용해서 가장 유력한 클래스를 예측
#
# 확률의 의해
# (사건의 확률) = (사건이 발생한 시행 횟수) / (전체 시행 횟수)
# 오늘과 비슷한 조건을 갖는 10일 중 3일 동안 비가 왔다면 비가 올 확률은 3/10 = 30%
# P(A): 사건 A의 확률
# 가능한 모든 시행 결과들의 확률의 합 = 1
# - 날씨라고 하면 rainy, sunny, crowd, snowy 의 발생 확률의 합 1
#
# 결합확률
# Spam에 주로 포함되는 단어
# 당일, 고객, 최저, 최대, 팀장, 캐피탈등이 포함
# 당일이라는 단어가 포함되었다고 전부 스팸은 아님
# 당일이라는 단어는 햄에도 포함되어 있음
#
# 독립 사건에서의 결합확률
# 우리가 알고 싶은 것은 당일 이라는 단어가 포함되어 있을 떄 스팸일 확률을 구하고 싶음
# P(스팸)과 P(당일) 이 독립적이라면 두 사건이 동시에 발생할 확률을 구하기 쉬움
# P(스팸 and 당일) = P(스팸) * P(당일)
# 전체 메세지중 스팸의 비율이 20%이고, 전체 이메일중 당일이 퐇마된 비율이 5%라고 하면
# 두 사건이 독립일 때 당일이 포함되어 있을 때 스팸의 가능성은 0.2 * 0.05 = 0.1임
# 하지만 이 둘이 독립이 아님 
#
# 베이즈 정리
# P(A|B) = P(A and B) / P(B) = P(A and B) * ( 1 / P(B))
# B가 발생환 상황에서 A가 발생할 확률 -> 조건부 확률(conditional probability)
# P(A and B) = P(B|A) * P(A)
# P(A|B) = P(A and B) / P(B) = P(B|A) * P(A)/P(B)
# A, B가 독립인 경우가 거의 없어, 곱확률을 구하기 어렵다.
# 따라서 P(A n B) 대신 P(B|A) * P(A) 를 사용하는 듯?!
#
# P(스팸 | 당일) = P(당일 | 스팸) * P(스팸) / P(당일)
# P(스팸 | 당일): 사후 확률(당일 단어가 등장한 사건이 발생하고 난후 스팸일 확률)
# P(당일 | 스팸): 우도(가능도)
# 이 변수가 "스팸이라는 모수" 에서 "당일" 이 나올 가능성을 나타내는 값이다.
# 우도는 모두 더해도 1이 나오지 않을 수 있음
# P(당일 | 스팸) + P(당일 | 햄) <= 1
# 확률이라는 개념과 조금 다르다 
#
# P(스팸 | 당일 n 최고 n 최저 n 팀장)
# 이렇게 여러 단어가 나오면 당일만 나온 경우보다 스팸일 가능성이 더 높아질텐데 이를 어떻게 계산할까?
# P(스팸 | 다일 n 최고 n 최저 n 팀장)
# = P(당일 n 최고 n 최저 n 팀장| 스팸) P(스팸) / P (당일 n 최고 n 최저 n 팀장)
# 모든 사건의 교집합에 대한 확률을 구하기는 어려움
# 엄쳥난 양의 메모리가 필요함
# 하지만 사건간의 독립을 가정하며 구하기 쉬워짐
# 즉, P(A n B) = P(A) * P(B)을 가정함
# 기하급수적인 메모리가 필요하다고 한다.

# Naive?  그냥 독립이라고 가정해버림 => P(A n B) = P(A)P(B)